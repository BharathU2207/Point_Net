{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:08.613973Z",
     "iopub.status.busy": "2025-01-08T09:38:08.612954Z",
     "iopub.status.idle": "2025-01-08T09:38:12.761174Z",
     "shell.execute_reply": "2025-01-08T09:38:12.760369Z",
     "shell.execute_reply.started": "2025-01-08T09:38:08.613914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys \n",
    "import json \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "\n",
    "# For plotting \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objects as go \n",
    "from plotly.subplots import make_subplots \n",
    "\n",
    "# append path to custom scripts \n",
    "sys.path.append('/kaggle/input/lidar-od-scripts/gpuVersion/gpuVersion/')\n",
    "\n",
    "# torch imports \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from visual_utils import plot_pc_data3d, plot_bboxes_3d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapenet Core Dataset Exploration \n",
    "\n",
    "- shapenet_core is a subset of the original ShapeNet dataset \n",
    "- It contains single clean 3D models, manually verified category and alignment annotations\n",
    "- 16 classes from 12 categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:12.763593Z",
     "iopub.status.busy": "2025-01-08T09:38:12.763091Z",
     "iopub.status.idle": "2025-01-08T09:38:12.870858Z",
     "shell.execute_reply": "2025-01-08T09:38:12.869793Z",
     "shell.execute_reply.started": "2025-01-08T09:38:12.763553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Path to data folder \n",
    "DATA_FOLDER = '/kaggle/input/shapenet-core-seg/Shapenetcore_benchmark/'\n",
    "\n",
    "class_name_id_map = {'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, \n",
    "                'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9,\n",
    "                'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, \n",
    "                'Skateboard': 14, 'Table': 15}\n",
    "\n",
    "class_id_name_map = {v:k for k,v in class_name_id_map.items()}\n",
    "\n",
    "PCD_SCENE=dict(xaxis=dict(visible=False), yaxis=dict(visible=False), zaxis=dict(visible=False), aspectmode='data')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:12.872383Z",
     "iopub.status.busy": "2025-01-08T09:38:12.872075Z",
     "iopub.status.idle": "2025-01-08T09:38:14.429395Z",
     "shell.execute_reply": "2025-01-08T09:38:14.428500Z",
     "shell.execute_reply.started": "2025-01-08T09:38:12.872349Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_4.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_split_data = json.load(open('/kaggle/input/shapenet-core-seg/Shapenetcore_benchmark/train_split.json', 'r'))\n",
    "train_class_count = np.array([x[0] for x in train_split_data])\n",
    "\n",
    "# plottin classwise count in the train set \n",
    "train_dist_plots = [go.Bar(x=list(class_name_id_map.keys()), y= np.bincount(train_class_count))]\n",
    "layout = dict(template=\"plotly_dark\", title=\"Shapenet Core Train Distribution\", title_x=0.5)\n",
    "fig = go.Figure(data=train_dist_plots, layout=layout)\n",
    "fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.430570Z",
     "iopub.status.busy": "2025-01-08T09:38:14.430290Z",
     "iopub.status.idle": "2025-01-08T09:38:14.656624Z",
     "shell.execute_reply": "2025-01-08T09:38:14.655800Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.430545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5263\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "points_list = glob.glob(\"/kaggle/input/shapenet-core-seg/Shapenetcore_benchmark/04379243/points/*.npy\")\n",
    "print(len(points_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.659262Z",
     "iopub.status.busy": "2025-01-08T09:38:14.658977Z",
     "iopub.status.idle": "2025-01-08T09:38:14.679760Z",
     "shell.execute_reply": "2025-01-08T09:38:14.678837Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.659235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points shape = (2685, 3), min xyz = [-0.25886 -0.16786 -0.38479], max xyz = [0.27159 0.16786 0.38475]\n",
      "seg_labels shape = (2685,), unique labels = [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.randint(0,len(points_list))\n",
    "\n",
    "# load point cloud data\n",
    "points = np.load(points_list[idx])\n",
    "print(f\"points shape = {points.shape}, min xyz = {np.min(points, axis=0)}, max xyz = {np.max(points, axis=0)}\")\n",
    "\n",
    "# load seg labels \n",
    "seg_file_path = points_list[idx].replace('points', 'points_label').replace('.npy', '.seg')\n",
    "seg_labels = np.loadtxt(seg_file_path).astype(np.int8)\n",
    "print(f\"seg_labels shape = {seg_labels.shape}, unique labels = {np.unique(seg_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.681168Z",
     "iopub.status.busy": "2025-01-08T09:38:14.680905Z",
     "iopub.status.idle": "2025-01-08T09:38:14.686079Z",
     "shell.execute_reply": "2025-01-08T09:38:14.685240Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.681143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# there is a maxof 16 parts in an object in Shapenet core Dataset \n",
    "# Let us create random colours according to part labels \n",
    "NUM_PARTS = 16\n",
    "PART_COLORS = np.random.choice(range(255),size=(NUM_PARTS,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.687971Z",
     "iopub.status.busy": "2025-01-08T09:38:14.687206Z",
     "iopub.status.idle": "2025-01-08T09:38:14.761064Z",
     "shell.execute_reply": "2025-01-08T09:38:14.760297Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.687932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pc_plots = plot_pc_data3d(x = points[:,0], y = points[:,1], z = points[:,2], apply_color_gradient=False, color = PART_COLORS[seg_labels -1], marker_size =2) \n",
    "layout = dict(template = 'plotly_dark', title = 'Raw Point cloud', scene = PCD_SCENE, title_x = 0.5) \n",
    "fig = go.Figure(data = pc_plots, layout = layout)\n",
    "fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Custom Dataset \n",
    "- Creating a dataset object\n",
    "- Creating a PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.762247Z",
     "iopub.status.busy": "2025-01-08T09:38:14.762007Z",
     "iopub.status.idle": "2025-01-08T09:38:14.769939Z",
     "shell.execute_reply": "2025-01-08T09:38:14.769174Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.762222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShapeNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, split_type, num_samples=2500):\n",
    "        self.root_dir = root_dir\n",
    "        self.split_type = split_type\n",
    "        self.num_samples = num_samples\n",
    "        with open(os.path.join(root_dir, f'{self.split_type}_split.json'), 'r') as f:\n",
    "            self.split_data = json.load(f) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # read point cloud data\n",
    "        class_id, class_name, point_cloud_path, seg_label_path = self.split_data[index]\n",
    "        \n",
    "        # Point cloud data \n",
    "        point_cloud_path = os.path.join(self.root_dir, point_cloud_path)\n",
    "        pc_data = np.load(point_cloud_path)\n",
    "\n",
    "        # Seg labels \n",
    "        # the -1 is to change part values from [1 - 16] to [0 - 15] \n",
    "        # this helps us run segmentation \n",
    "        pc_seg_labels = np.loadtxt(os.path.join(self.root_dir, seg_label_path)).astype(np.int8) - 1\n",
    "        # Sample a fixed number of points \n",
    "        num_points = pc_data.shape[0]\n",
    "        if num_points < self.num_samples:\n",
    "            additional_indices = np.random.choice(num_points, self.num_samples - num_points, replace=True)\n",
    "            pc_data = np.concatenate((pc_data, pc_data[additional_indices]), axis=0)\n",
    "            pc_seg_labels = np.concatenate((pc_seg_labels, pc_seg_labels[additional_indices]), axis=0)\n",
    "        else: \n",
    "            # Randomly select max_num_point samples from the available points \n",
    "            random_indices = np.random.choice(num_points, self.num_samples)\n",
    "            pc_data = pc_data[random_indices]\n",
    "            pc_seg_labels = pc_seg_labels[random_indices]\n",
    "\n",
    "        # return variables \n",
    "        data_dict= {}\n",
    "        data_dict['class_id'] = class_id\n",
    "        data_dict['class_name'] = class_name        \n",
    "        data_dict['points'] = pc_data \n",
    "        data_dict['seg_labels'] = pc_seg_labels \n",
    "        return data_dict   \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.771245Z",
     "iopub.status.busy": "2025-01-08T09:38:14.770988Z",
     "iopub.status.idle": "2025-01-08T09:38:14.822161Z",
     "shell.execute_reply": "2025-01-08T09:38:14.821388Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.771214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length = 12137\n",
      "Validation set length = 1861\n",
      "Test set length = 2848\n"
     ]
    }
   ],
   "source": [
    "train_set = ShapeNetDataset(root_dir = DATA_FOLDER, split_type='train')\n",
    "val_set = ShapeNetDataset(root_dir = DATA_FOLDER, split_type='val')\n",
    "test_set = ShapeNetDataset(root_dir = DATA_FOLDER, split_type='test')\n",
    "print(f\"Train set length = {len(train_set)}\")\n",
    "print(f\"Validation set length = {len(val_set)}\")\n",
    "print(f\"Test set length = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.823178Z",
     "iopub.status.busy": "2025-01-08T09:38:14.822963Z",
     "iopub.status.idle": "2025-01-08T09:38:14.838675Z",
     "shell.execute_reply": "2025-01-08T09:38:14.837885Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.823156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset sample = ['class_id', 'class_name', 'points', 'seg_labels']\n",
      "class_id = 15, class_name = Table\n"
     ]
    }
   ],
   "source": [
    "data_dict= train_set[25]\n",
    "print(f\"Keys in dataset sample = {list(data_dict.keys())}\")\n",
    "points = data_dict['points']\n",
    "seg_labels = data_dict['seg_labels']\n",
    "print(f\"class_id = {data_dict['class_id']}, class_name = {data_dict['class_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.839842Z",
     "iopub.status.busy": "2025-01-08T09:38:14.839602Z",
     "iopub.status.idle": "2025-01-08T09:38:14.902937Z",
     "shell.execute_reply": "2025-01-08T09:38:14.902086Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.839819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pc_plots = plot_pc_data3d(x=points[:,0], y=points[:,1], z=points[:,2], apply_color_gradient=False, color=PART_COLORS[seg_labels - 1], marker_size=2)\n",
    "layout = dict(template=\"plotly_dark\", title=f\"{data_dict['class_name']}, class id = {data_dict['class_id']}, from Shapenetcore Torch Dataset\", scene=PCD_SCENE, title_x=0.5)\n",
    "fig = go.Figure(data=pc_plots, layout=layout)      \n",
    "fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader for Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.904391Z",
     "iopub.status.busy": "2025-01-08T09:38:14.904034Z",
     "iopub.status.idle": "2025-01-08T09:38:14.910153Z",
     "shell.execute_reply": "2025-01-08T09:38:14.909364Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.904350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch_list):\n",
    "    ret = {}\n",
    "    ret['class_id'] =  torch.from_numpy(np.array([x['class_id'] for x in batch_list])).long()\n",
    "    ret['class_name'] = np.array([x['class_name'] for x in batch_list])\n",
    "    ret['points'] = torch.from_numpy(np.stack([x['points'] for x in batch_list], axis=0)).float()\n",
    "    ret['seg_labels'] = torch.from_numpy(np.stack([x['seg_labels'] for x in batch_list], axis=0)).long()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:14.912127Z",
     "iopub.status.busy": "2025-01-08T09:38:14.911350Z",
     "iopub.status.idle": "2025-01-08T09:38:15.613131Z",
     "shell.execute_reply": "2025-01-08T09:38:15.611922Z",
     "shell.execute_reply.started": "2025-01-08T09:38:14.912099Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['class_id', 'class_name', 'points', 'seg_labels'])\n",
      "batch_dict[points].shape = torch.Size([16, 2500, 3])\n",
      "batch_dict[seg_labels].shape = torch.Size([16, 2500])\n",
      "batch_dict[class_id].shape = torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Testing loader \n",
    "sample_loader = torch.utils.data.DataLoader(train_set, batch_size=16, num_workers=2, shuffle=True, collate_fn=collate_fn) \n",
    "dataloader_iter = iter(sample_loader)   \n",
    "batch_dict = next(dataloader_iter)\n",
    "print(batch_dict.keys())\n",
    "for key in ['points','seg_labels', 'class_id']:\n",
    "    print(f\"batch_dict[{key}].shape = {batch_dict[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:15.619322Z",
     "iopub.status.busy": "2025-01-08T09:38:15.618925Z",
     "iopub.status.idle": "2025-01-08T09:38:15.625560Z",
     "shell.execute_reply": "2025-01-08T09:38:15.624610Z",
     "shell.execute_reply.started": "2025-01-08T09:38:15.619267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batchSize= 32\n",
    "workers = 2\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchSize, shuffle=True, num_workers=workers, collate_fn=collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batchSize, shuffle=True, num_workers=workers, collate_fn=collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batchSize,shuffle=True, num_workers=workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointNet \n",
    "Architecture taken from [this paper](https://arxiv.org/pdf/1612.00593)\n",
    "In depth implementation and explaination can be found in [this notebook](https://github.com/BharathU2207/Point_Net/blob/main/PointNet_architecture_from_Scratch.ipynb)\n",
    "\n",
    "The PointNet comprises several parts \n",
    "- T-Nets\n",
    "- FeatureNet\n",
    "- Classification or Segmentation Head "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:15.627516Z",
     "iopub.status.busy": "2025-01-08T09:38:15.626689Z",
     "iopub.status.idle": "2025-01-08T09:38:15.640504Z",
     "shell.execute_reply": "2025-01-08T09:38:15.639761Z",
     "shell.execute_reply.started": "2025-01-08T09:38:15.627481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:15.641870Z",
     "iopub.status.busy": "2025-01-08T09:38:15.641554Z",
     "iopub.status.idle": "2025-01-08T09:38:15.652114Z",
     "shell.execute_reply": "2025-01-08T09:38:15.651193Z",
     "shell.execute_reply.started": "2025-01-08T09:38:15.641835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setting up the T net class\n",
    "class Tnet(nn.Module):\n",
    "  ''' T-Net learns a transformation matrix with a specified dimension'''\n",
    "  def __init__(self, dim, num_points = 2500):\n",
    "    super(Tnet, self).__init__()\n",
    "\n",
    "    # Dimensions for transform matrix\n",
    "    self.dim = dim\n",
    "\n",
    "    self.conv1 = nn.Conv1d(dim, 64, kernel_size = 1)\n",
    "    self.conv2 = nn.Conv1d(64, 128, kernel_size =1)\n",
    "    self.conv3 = nn.Conv1d(128, 1024, kernel_size =1)\n",
    "\n",
    "    self.linear1 = nn.Linear(1024, 512)\n",
    "    self.linear2 = nn.Linear(512, 256)\n",
    "    self.linear3 = nn.Linear(256, dim**2) # This allows the class to be used for both input spaces\n",
    "\n",
    "    self.bn1 = nn.BatchNorm1d(64)\n",
    "    self.bn2 = nn.BatchNorm1d(128)\n",
    "    self.bn3 = nn.BatchNorm1d(1024)\n",
    "    self.bn4 = nn.BatchNorm1d(512)\n",
    "    self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "    self.max_pool = nn.MaxPool1d(kernel_size = num_points)\n",
    "\n",
    "  def forward(self, x):\n",
    "    bs = x.shape[0]\n",
    "\n",
    "    # pass through shared MLP layers (conv1d)\n",
    "    x = self.bn1(F.relu(self.conv1(x)))\n",
    "    x = self.bn2(F.relu(self.conv2(x)))\n",
    "    x = self.bn3(F.relu(self.conv3(x)))\n",
    "\n",
    "    # max pool over num points\n",
    "    x = self.max_pool(x).view(bs, -1)\n",
    "    '''.view(bs , -1) flattens the output of the maxpool from a 3D tensor [bs, num_channels, 1] to\n",
    "    a 2d tensor [bs, num_features]. The -1 tells PyTorch to calculate the total number of rows\n",
    "    to ensure that the number of elements is consistent.\n",
    "    '''\n",
    "\n",
    "    # Pass through the MLP\n",
    "    x = self.bn4(F.relu(self.linear1(x)))\n",
    "    x = self.bn5(F.relu(self.linear2(x)))\n",
    "    x = self.linear3(x)\n",
    "\n",
    "    # initialize the identity matrix\n",
    "    iden = torch.eye(self.dim, requires_grad = True).repeat(bs ,1, 1)\n",
    "\n",
    "    if x.is_cuda:\n",
    "      iden = iden.cuda()\n",
    "\n",
    "    x = x.view(-1, self.dim, self.dim) + iden # reshaping output to a matrix\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:15.653404Z",
     "iopub.status.busy": "2025-01-08T09:38:15.653126Z",
     "iopub.status.idle": "2025-01-08T09:38:16.693609Z",
     "shell.execute_reply": "2025-01-08T09:38:16.692715Z",
     "shell.execute_reply.started": "2025-01-08T09:38:15.653379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNet torch.Size([32, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "test_model = Tnet(3).to(device) \n",
    "sim_data = Variable(torch.rand(32, 3, 2500)).to(device) \n",
    "out = test_model(sim_data) \n",
    "print('TNet', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet Backbone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:16.696963Z",
     "iopub.status.busy": "2025-01-08T09:38:16.696239Z",
     "iopub.status.idle": "2025-01-08T09:38:16.709343Z",
     "shell.execute_reply": "2025-01-08T09:38:16.708353Z",
     "shell.execute_reply.started": "2025-01-08T09:38:16.696933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Point Net Backbone\n",
    "class PointNetBackbone(nn.Module):\n",
    "  def __init__(self, num_points = 2500, num_global_feats = 1024, local_feat = True):\n",
    "    super(PointNetBackbone, self).__init__()\n",
    "\n",
    "    # if true concat local and global features\n",
    "    self.num_points = num_points\n",
    "    self.num_global_feats = num_global_feats\n",
    "    self.local_feat = local_feat\n",
    "\n",
    "    # spatial transformer network (T-Net)\n",
    "    self.tnet1 = Tnet(dim = 3, num_points = num_points)\n",
    "    self.tnet2 = Tnet(dim = 64, num_points = num_points)\n",
    "\n",
    "    # Shared MLP 1\n",
    "    self.conv1 = nn.Conv1d(3, 64, kernel_size =1)\n",
    "    self.conv2 = nn.Conv1d(64, 64, kernel_size =1)\n",
    "\n",
    "    # Shared MLP 2\n",
    "    self.conv3 = nn.Conv1d(64, 64, kernel_size =1)\n",
    "    self.conv4 = nn.Conv1d(64, 128, kernel_size =1)\n",
    "    self.conv5 = nn.Conv1d(128, self.num_global_feats, kernel_size = 1)\n",
    "\n",
    "    # batch norms for both shared mlps\n",
    "    self.bn1 = nn.BatchNorm1d(64)\n",
    "    self.bn2 = nn.BatchNorm1d(64)\n",
    "    self.bn3 = nn.BatchNorm1d(64)\n",
    "    self.bn4 = nn.BatchNorm1d(128)\n",
    "    self.bn5 = nn.BatchNorm1d(self.num_global_feats)\n",
    "\n",
    "\n",
    "    # Max pool to get the global features\n",
    "    # We can visualize them by getting the max pool func to return the indices by setting return_indices = True\n",
    "    self.max_pool = nn.MaxPool1d(kernel_size= num_points, return_indices = True)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # get batch size\n",
    "    bs = x.shape[0]\n",
    "\n",
    "    # pass through first Tnet to get transform matrix\n",
    "    A_input = self.tnet1(x)\n",
    "\n",
    "    # perform first transformation across each point in the batch\n",
    "    x = torch.bmm(x.transpose(2, 1), A_input).transpose(2,1)\n",
    "\n",
    "    # pass through first shared MLP\n",
    "    x = self.bn1(F.relu(self.conv1(x)))\n",
    "    x = self.bn2(F.relu(self.conv2(x)))\n",
    "\n",
    "    # get feature transform\n",
    "    A_feat = self.tnet2(x)\n",
    "\n",
    "    # perform second transformation across each (64 dim) features in the batch\n",
    "    x = torch.bmm(x.transpose(2,1), A_feat).transpose(2,1)\n",
    "\n",
    "    # store local point features for segmentation head\n",
    "    local_features = x.clone()\n",
    "\n",
    "    # pass through second MLP head\n",
    "    x = self.bn3(F.relu(self.conv3(x)))\n",
    "    x = self.bn4(F.relu(self.conv4(x)))\n",
    "    x = self.bn5(F.relu(self.conv5(x)))\n",
    "\n",
    "    # get global feature vector and critial indexes\n",
    "    global_features, critical_indexes = self.max_pool(x)\n",
    "    global_features = global_features.view(bs, -1)\n",
    "    critical_indexes = critical_indexes.view(bs, -1)\n",
    "\n",
    "    if self.local_feat:\n",
    "      features = torch.cat((local_features, global_features.unsqueeze(-1).repeat(1, 1, self.num_points)), dim =1)\n",
    "      return features, critical_indexes, A_feat\n",
    "    else:\n",
    "      return global_features, critical_indexes, A_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:16.710692Z",
     "iopub.status.busy": "2025-01-08T09:38:16.710438Z",
     "iopub.status.idle": "2025-01-08T09:38:17.026283Z",
     "shell.execute_reply": "2025-01-08T09:38:17.025390Z",
     "shell.execute_reply.started": "2025-01-08T09:38:16.710667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global feat torch.Size([32, 1024])\n",
      "point feat torch.Size([32, 1088, 2500])\n"
     ]
    }
   ],
   "source": [
    "pointbackbone = PointNetBackbone(local_feat = False).to(device) \n",
    "out, _, _ = pointbackbone (sim_data) \n",
    "print('global feat', out.size())\n",
    "\n",
    "pointbackbone = PointNetBackbone(local_feat = True).to(device) \n",
    "out, _, _ = pointbackbone(sim_data)\n",
    "print('point feat', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.029934Z",
     "iopub.status.busy": "2025-01-08T09:38:17.029226Z",
     "iopub.status.idle": "2025-01-08T09:38:17.037563Z",
     "shell.execute_reply": "2025-01-08T09:38:17.036391Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.029897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Classification head\n",
    "class PointNetClassHead(nn.Module):\n",
    "  '''Classification Head'''\n",
    "  def __init__(self, num_points = 2500, num_global_feats = 1024, k =2):\n",
    "    super(PointNetClassHead, self).__init__()\n",
    "\n",
    "    # get the backbone (require only global features for classification)\n",
    "    self.backbone = PointNetBackbone(num_points, num_global_feats, local_feat = False)\n",
    "\n",
    "    # MLP for classification\n",
    "    self.linear1 = nn.Linear(num_global_feats, 512)\n",
    "    self.linear2 = nn.Linear(512, 256)\n",
    "    self.linear3 = nn.Linear(256, k)\n",
    "\n",
    "    # batchnorm for the first 2 linear layers\n",
    "    self.bn1 = nn.BatchNorm1d(512)\n",
    "    self.bn2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    # the paper says only batchnorm is added to the layers before the classification layer\n",
    "    # but another version uses dropouts for the first 2 layers as well\n",
    "    self.dropout = nn.Dropout(p = 0.3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get global features\n",
    "    x, crit_idxs, A_feat = self.backbone(x)\n",
    "    x = self.bn1(F.relu(self.linear1(x)))\n",
    "    x = self.bn2(F.relu(self.linear2(x)))\n",
    "    x = self.dropout(x)\n",
    "    x = self.linear3(x)\n",
    "\n",
    "    return x, crit_idxs, A_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.039305Z",
     "iopub.status.busy": "2025-01-08T09:38:17.038776Z",
     "iopub.status.idle": "2025-01-08T09:38:17.194465Z",
     "shell.execute_reply": "2025-01-08T09:38:17.193561Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.039273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "cls = PointNetClassHead(k = 16).to(device) \n",
    "out, _, _ = cls(sim_data) \n",
    "print('class', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss for Point Net: Focal Loss \n",
    "\n",
    "For Point Net we will be using the Categorical Cross Entropy loss with a regularization term that will enforce the high dimensional transform matrix to 0. We will also present the option for using the Balanced Cross Entropy Loss via the 'alpha' argument which assigns a weight to each class this weights the importance of each example based on their class frequencies. We also provide the option to use the Focal Loss which adds a modulating term to the Cross Entropy Loss $(1-p_n)^γ$, this term forces the model to focus on hard examples (i.e. examples with low prediction probability). Some notes on the Focal Loss are given below. \n",
    "\n",
    "**Focal Loss**\n",
    "The Focal Loss is a modified Cross Entropy (CE) Loss. The CE Loss for a sample *n* is given below. \n",
    "\n",
    "$$CE(s_n, y_n) = -α_{y_n} * log{(\\frac{exp(s_n)}{\\sum_{i=1}^{N}})} 1c(y_n)$$\n",
    "\n",
    "Where **$s_n$** is the predicted class score vector (logits), **$M$** is the number of classes, **$y_n$** is the true class, **$α_{y_n}$** is the class weight, and **$1c(y_n)$** is the [Indcator Function](https://en.wikipedia.org/wiki/Indicator_function) that tells us to only consider the prediction for the current class **$y_n$**.\n",
    "\n",
    "We may also notice that the term inside the **$log$** is the Softmax function, which along wth the indicator function, gives us the predicted class probability. We can rewrite the CE Loss in a more simple format as:\n",
    "$$CE(p_n) = -α_{y_n}log(p_n)$$\n",
    "Where the **$n$** subscript refers to the true class at sample **$n$**. \n",
    "The CE loss is typically unweighted, but if a weight is used it is reffered to as the Balanced (or Weighted) CE Loss. The weights are usually based on inverse class distribution and typically range from \\[0, 1\\]. The weights can also be set as a hyperparameter via Cross Validation.\n",
    "$$α = \\frac{1}{class\\ counts}$$\n",
    "We may also normalize alpha so that it spreads between \\[0,1\\]\n",
    "$$α = \\frac{α}{max\\ α}$$\n",
    "The Focal Loss adds an additional modulating factor to the weighted CE Loss: $(1-p_n)^γ$, where $γ>=0$ is referred to as the focusing parameter. This term tends to 0 when the prediction probability is high, and has a larger value when the prediction probability is lower forcing the model to focus on the hard examples. It can be said that the focusing parameter smoothly adjusts the rate at which easy examples are down-weighted. We can formally express the Focal Loss as: \n",
    "$$FL(p_n) = -α_{y_n}(1-p_n)^γlog(p_n)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.196024Z",
     "iopub.status.busy": "2025-01-08T09:38:17.195734Z",
     "iopub.status.idle": "2025-01-08T09:38:17.209515Z",
     "shell.execute_reply": "2025-01-08T09:38:17.208590Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.195996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# special loss for Classification: Focal Loss + regularization\n",
    "class PointNetLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=0, reg_weight=0, size_average=True):\n",
    "        super(PointNetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reg_weight = reg_weight\n",
    "        self.size_average = size_average\n",
    "\n",
    "        # sanitize inputs\n",
    "        if isinstance(alpha,(float, int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,(list, np.ndarray)): self.alpha = torch.Tensor(alpha)\n",
    "\n",
    "        # get Balanced Cross Entropy Loss\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(weight=self.alpha)\n",
    "        \n",
    "\n",
    "    def forward(self, predictions, targets, A=None):\n",
    "\n",
    "        # get batch size\n",
    "        bs = predictions.size(0)\n",
    "\n",
    "        # get Balanced Cross Entropy Loss\n",
    "        ce_loss = self.cross_entropy_loss(predictions, targets)\n",
    "\n",
    "        # get predicted class probabilities for the true class\n",
    "        pn = F.softmax(predictions, -1)\n",
    "        pn = pn.gather(1, targets.view(-1, 1)).view(-1)\n",
    "\n",
    "        # get regularization term\n",
    "        if self.reg_weight > 0:\n",
    "            I = torch.eye(64).unsqueeze(0).repeat(A.shape[0], 1, 1) # .to(device)\n",
    "            if A.is_cuda: I = I.cuda()\n",
    "            reg = torch.linalg.norm(I - torch.bmm(A, A.transpose(2, 1)))\n",
    "            reg = self.reg_weight*reg/bs\n",
    "        else:\n",
    "            reg = 0\n",
    "\n",
    "        # compute loss (negative sign is included in ce_loss)\n",
    "        loss = ((1 - pn)**self.gamma * ce_loss)\n",
    "        if self.size_average: return loss.mean() + reg\n",
    "        else: return loss.sum() + reg\n",
    "\n",
    "\n",
    "# special loss for segmentation Focal Loss + Dice Loss\n",
    "class PointNetSegLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=0, size_average=True, dice=False):\n",
    "        super(PointNetSegLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.dice = dice\n",
    "\n",
    "        # sanitize inputs\n",
    "        if isinstance(alpha,(float, int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,(list, np.ndarray)): self.alpha = torch.Tensor(alpha)\n",
    "\n",
    "        # get Balanced Cross Entropy Loss\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(weight=self.alpha)\n",
    "        \n",
    "    def forward(self, predictions, targets, pred_choice=None):\n",
    "        # print(f\"Predictions(loss fn) shape before: {predictions.shape}\")\n",
    "        # print(f\"Targets(loss fn) shape before: {targets.shape}\")\n",
    "    \n",
    "        # get Balanced Cross Entropy Loss\n",
    "        ce_loss = self.cross_entropy_loss(predictions.transpose(2, 1), targets)  # Reshape targets to [B, N]\n",
    "\n",
    "        # reformat predictions (b, n, c) -> (b*n, c)\n",
    "        predictions = predictions.contiguous() \\\n",
    "                                 .view(-1, predictions.size(2)) \n",
    "        # get predicted class probabilities for the true class\n",
    "        pn = F.softmax(predictions, dim = -1)\n",
    "        pn = pn.gather(1, targets.view(-1, 1)).view(-1)\n",
    "\n",
    "        # compute loss (negative sign is included in ce_loss)\n",
    "        loss = ((1 - pn)**self.gamma * ce_loss)\n",
    "        if self.size_average: loss = loss.mean() \n",
    "        else: loss = loss.sum()\n",
    "\n",
    "        # add dice coefficient if necessary\n",
    "        if self.dice: return loss + self.dice_loss(targets, pred_choice, eps=1)\n",
    "        else: return loss\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_loss(predictions, targets, eps=1):\n",
    "        ''' Compute Dice loss, directly compare predictions with truth '''\n",
    "\n",
    "        targets = targets.reshape(-1)\n",
    "        predictions = predictions.reshape(-1)\n",
    "\n",
    "        cats = torch.unique(targets)\n",
    "\n",
    "        top = 0\n",
    "        bot = 0\n",
    "        for c in cats:\n",
    "            locs = targets == c\n",
    "\n",
    "            # get truth and predictions for each class\n",
    "            y_tru = targets[locs]\n",
    "            y_hat = predictions[locs]\n",
    "\n",
    "            top += torch.sum(y_hat == y_tru)\n",
    "            bot += len(y_tru) + len(y_hat)\n",
    "\n",
    "\n",
    "        return 1 - 2*((top + eps)/(bot + eps)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.211579Z",
     "iopub.status.busy": "2025-01-08T09:38:17.211097Z",
     "iopub.status.idle": "2025-01-08T09:38:17.233831Z",
     "shell.execute_reply": "2025-01-08T09:38:17.233127Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.211484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, criterion, optimizer, dataloader_train,\n",
    "                label_str = 'class_id', lr_scheduler = None, output_name = 'pointnet.pth', Segmentation = False):\n",
    "    # move model to device\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting {epoch + 1} epoch ...\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        if Segmentation == False: \n",
    "            for batch_dict in tqdm(dataloader_train, total=len(dataloader_train)):            \n",
    "                # Forward pass\n",
    "                x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "                labels = batch_dict[label_str].to(device).view(-1)\n",
    "                \n",
    "                pred, _, A = model(x)\n",
    "                loss = criterion(pred, labels, A)\n",
    "                train_loss += loss.item()\n",
    "                  \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "                # adjusting learning rate\n",
    "                if lr_scheduler is not None:\n",
    "                    lr_scheduler.step()\n",
    "        elif Segmentation == True: \n",
    "            for batch_dict in tqdm(dataloader_train, total=len(dataloader_train)): \n",
    "                x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "                #targets = batch_dict[label_str].squeeze().to(device) \n",
    "                labels = batch_dict[label_str].to(device)\n",
    "\n",
    "                preds, _, _ = model(x)\n",
    "                preds_trans = preds.transpose(2,1)\n",
    "\n",
    "                pred_choice = torch.softmax(preds_trans, dim=2).argmax(dim=2)\n",
    "\n",
    "                loss = criterion(preds_trans, labels, pred_choice)\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if lr_scheduler is not None: \n",
    "                    lr_scheduler.step() \n",
    "                \n",
    "        # compute per batch losses, metric value\n",
    "        train_loss = train_loss / len(dataloader_train)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, trainLoss:{train_loss:6.5f}')\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.save(model.state_dict(), output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.235021Z",
     "iopub.status.busy": "2025-01-08T09:38:17.234771Z",
     "iopub.status.idle": "2025-01-08T09:38:17.248600Z",
     "shell.execute_reply": "2025-01-08T09:38:17.247857Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.234994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# def report_gpu(): \n",
    "#     print(torch.cuda.list_gpu_processes()) \n",
    "#     gc.collect() \n",
    "#     torch.cuda.empty_cache()\n",
    "# report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:38:17.250009Z",
     "iopub.status.busy": "2025-01-08T09:38:17.249554Z",
     "iopub.status.idle": "2025-01-08T10:12:51.151608Z",
     "shell.execute_reply": "2025-01-08T10:12:51.150444Z",
     "shell.execute_reply.started": "2025-01-08T09:38:17.249971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:19<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, trainLoss:0.53907\n",
      "Starting 2 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:19<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, trainLoss:0.13574\n",
      "Starting 3 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:21<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, trainLoss:0.08899\n",
      "Starting 4 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:22<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, trainLoss:0.06769\n",
      "Starting 5 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:22<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, trainLoss:0.05557\n",
      "Starting 6 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, trainLoss:0.04260\n",
      "Starting 7 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, trainLoss:0.04066\n",
      "Starting 8 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, trainLoss:0.04848\n",
      "Starting 9 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, trainLoss:0.04504\n",
      "Starting 10 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, trainLoss:0.04752\n",
      "Starting 11 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, trainLoss:0.03438\n",
      "Starting 12 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, trainLoss:0.02757\n",
      "Starting 13 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, trainLoss:0.02569\n",
      "Starting 14 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, trainLoss:0.02022\n",
      "Starting 15 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, trainLoss:0.02364\n",
      "Starting 16 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, trainLoss:0.02133\n",
      "Starting 17 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, trainLoss:0.01877\n",
      "Starting 18 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, trainLoss:0.02316\n",
      "Starting 19 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, trainLoss:0.01928\n",
      "Starting 20 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, trainLoss:0.01663\n",
      "Starting 21 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, trainLoss:0.01790\n",
      "Starting 22 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, trainLoss:0.01468\n",
      "Starting 23 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, trainLoss:0.01275\n",
      "Starting 24 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, trainLoss:0.01477\n",
      "Starting 25 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [01:23<00:00,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, trainLoss:0.01582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "N_EPOCHS = 25\n",
    "LR = 0.01\n",
    "REG_WEIGHT = 0.001 \n",
    "\n",
    "num_points = 2500\n",
    "num_classes = 16\n",
    "\n",
    "# manually downweight the high frequency classes\n",
    "alpha = np.ones(num_classes)\n",
    "alpha[0] = 0.5  # airplane\n",
    "alpha[4] = 0.5  # chair\n",
    "alpha[-1] = 0.5 # table\n",
    "\n",
    "gamma = 2\n",
    "\n",
    "#criterion = nn.NLLLoss()\n",
    "criterion = PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=REG_WEIGHT).to(device)\n",
    "\n",
    "# create model, optimizer, lr_scheduler and pass to training function\n",
    "num_classes = len(class_id_name_map.items())\n",
    "classifier = PointNetClassHead(k = num_classes, num_points = num_points)\n",
    "\n",
    "# DEFINE OPTIMIZERS\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=LR, momentum=0.9)\n",
    "if torch.cuda.is_available():\n",
    "    classifier.cuda()\n",
    "_ = train_model(classifier, N_EPOCHS, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:12:51.154610Z",
     "iopub.status.busy": "2025-01-08T10:12:51.153551Z",
     "iopub.status.idle": "2025-01-08T10:13:12.269869Z",
     "shell.execute_reply": "2025-01-08T10:13:12.268749Z",
     "shell.execute_reply.started": "2025-01-08T10:12:51.154564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:21<00:00,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05312892890880617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = PointNetClassHead(k = num_classes).to(device) \n",
    "classifier.load_state_dict(torch.load('pointnet.pth', weights_only = True))\n",
    "classifier.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_dict in tqdm(test_loader, total=len(test_loader)):\n",
    "        x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "        labels = batch_dict['class_id'].to(device)\n",
    "        pred, _, A = classifier(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(pred, labels, A)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "evaluation_loss = total_loss / len(test_loader)\n",
    "print(evaluation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on individual items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:13:12.272665Z",
     "iopub.status.busy": "2025-01-08T10:13:12.272345Z",
     "iopub.status.idle": "2025-01-08T10:13:12.602195Z",
     "shell.execute_reply": "2025-01-08T10:13:12.601390Z",
     "shell.execute_reply.started": "2025-01-08T10:13:12.272633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"420\"\n",
       "    src=\"iframe_figures/figure_28.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random test sample\n",
    "test_sample = test_set[np.random.choice(np.arange(len(test_set)))]\n",
    "batch_dict = collate_fn([test_sample])\n",
    "x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "\n",
    "# Get model preds \n",
    "model_preds, _, _ = classifier(x)\n",
    "predicted_class = torch.argmax(model_preds, axis=1).detach().cpu().numpy()[0]\n",
    "predicted_class_name = class_id_name_map[predicted_class]\n",
    "pred_class_probs = F.softmax(model_preds.flatten(), dim=0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# plot results \n",
    "title = f\"Label = {test_sample['class_name']}, Predicted class = {predicted_class_name}\"\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"scatter3d\"}, {}]], column_widths=[0.4, 0.6])\n",
    "fig.update_layout(template=\"plotly_dark\", scene=PCD_SCENE, height = 400, width = 1200,\n",
    "                title=title, title_x=0.1, title_y=0.97, margin=dict(r=0, b=0, l=0, t=0))    \n",
    "fig.add_trace(plot_pc_data3d(x=test_sample['points'][:,0], y=test_sample['points'][:,1], z=test_sample['points'][:,2]), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=list(class_name_id_map.keys()), y=pred_class_probs, showlegend=False), row=1, col=2)\n",
    "fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:13:12.603631Z",
     "iopub.status.busy": "2025-01-08T10:13:12.603300Z",
     "iopub.status.idle": "2025-01-08T10:13:12.610927Z",
     "shell.execute_reply": "2025-01-08T10:13:12.610067Z",
     "shell.execute_reply.started": "2025-01-08T10:13:12.603602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Segmentation head\n",
    "class PointNetSegHead(nn.Module):\n",
    "  '''Segmentation Head'''\n",
    "  def __init__(self, num_points = 2500, num_global_feats = 1024, m =2):\n",
    "    super(PointNetSegHead, self).__init__()\n",
    "\n",
    "    self.num_points = num_points\n",
    "    self.m = m \n",
    "\n",
    "    # get the backbone\n",
    "    self.backbone = PointNetBackbone(num_points, num_global_feats, local_feat = True)\n",
    "\n",
    "    # Shared MLP\n",
    "    num_features = num_global_feats + 64 # local and global features\n",
    "    self.conv1 = nn.Conv1d(num_features, 512, kernel_size =1)\n",
    "    self.conv2 = nn.Conv1d(512, 256, kernel_size =1)\n",
    "    self.conv3 = nn.Conv1d(256, 128, kernel_size =1)\n",
    "    self.conv4 = nn.Conv1d(128, self.m, kernel_size =1)\n",
    "\n",
    "    # batch norm for shared MLP\n",
    "    self.bn1 = nn.BatchNorm1d(512)\n",
    "    self.bn2 = nn.BatchNorm1d(256)\n",
    "    self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    # get combined features\n",
    "    x, crit_idxs, A_feat = self.backbone(x)\n",
    "\n",
    "    # pass through shared MLP\n",
    "    x = self.bn1(F.relu(self.conv1(x)))\n",
    "    x = self.bn2(F.relu(self.conv2(x)))\n",
    "    x = self.bn3(F.relu(self.conv3(x)))\n",
    "    x = self.conv4(x)\n",
    "\n",
    "    #x = x.transpose(2,1)\n",
    "    return x, crit_idxs, A_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:13:12.612589Z",
     "iopub.status.busy": "2025-01-08T10:13:12.612018Z",
     "iopub.status.idle": "2025-01-08T10:13:12.755295Z",
     "shell.execute_reply": "2025-01-08T10:13:12.754386Z",
     "shell.execute_reply.started": "2025-01-08T10:13:12.612549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointNetSegHead(\n",
      "  (backbone): PointNetBackbone(\n",
      "    (tnet1): Tnet(\n",
      "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (linear3): Linear(in_features=256, out_features=9, bias=True)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (max_pool): MaxPool1d(kernel_size=2500, stride=2500, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (tnet2): Tnet(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (linear3): Linear(in_features=256, out_features=4096, bias=True)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (max_pool): MaxPool1d(kernel_size=2500, stride=2500, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv4): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv5): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (max_pool): MaxPool1d(kernel_size=2500, stride=2500, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv1): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv4): Conv1d(128, 16, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "seg torch.Size([32, 16, 2500])\n",
      "torch.Size([32, 2500, 16])\n"
     ]
    }
   ],
   "source": [
    "seg = PointNetSegHead(m = 16).to(device)\n",
    "print(seg) \n",
    "out, _, _ = seg(sim_data)\n",
    "print('seg', out.size()) \n",
    "preds_trans = out.transpose(2,1) \n",
    "print(preds_trans.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T10:13:12.756762Z",
     "iopub.status.busy": "2025-01-08T10:13:12.756430Z",
     "iopub.status.idle": "2025-01-08T11:08:31.579127Z",
     "shell.execute_reply": "2025-01-08T11:08:31.578079Z",
     "shell.execute_reply.started": "2025-01-08T10:13:12.756734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, trainLoss:0.66986\n",
      "Starting 2 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, trainLoss:0.28887\n",
      "Starting 3 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, trainLoss:0.22959\n",
      "Starting 4 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, trainLoss:0.21539\n",
      "Starting 5 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, trainLoss:0.19228\n",
      "Starting 6 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, trainLoss:0.17955\n",
      "Starting 7 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, trainLoss:0.16881\n",
      "Starting 8 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, trainLoss:0.16282\n",
      "Starting 9 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, trainLoss:0.16927\n",
      "Starting 10 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, trainLoss:0.15342\n",
      "Starting 11 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, trainLoss:0.14834\n",
      "Starting 12 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, trainLoss:0.13970\n",
      "Starting 13 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, trainLoss:0.13823\n",
      "Starting 14 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, trainLoss:0.13815\n",
      "Starting 15 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, trainLoss:0.13223\n",
      "Starting 16 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, trainLoss:0.14157\n",
      "Starting 17 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, trainLoss:0.12686\n",
      "Starting 18 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, trainLoss:0.12601\n",
      "Starting 19 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, trainLoss:0.13444\n",
      "Starting 20 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, trainLoss:0.12465\n",
      "Starting 21 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, trainLoss:0.12070\n",
      "Starting 22 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, trainLoss:0.11963\n",
      "Starting 23 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, trainLoss:0.12285\n",
      "Starting 24 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, trainLoss:0.11759\n",
      "Starting 25 epoch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [02:12<00:00,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, trainLoss:0.11897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 25\n",
    "num_points = 2500\n",
    "\n",
    "# manually set alpha weights\n",
    "alpha = np.ones(num_classes)\n",
    "alpha[0:3] *= 0.25 # balance background classes\n",
    "alpha[-1] *= 0.75  # balance clutter class\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "criterion = PointNetSegLoss(alpha=alpha, gamma=gamma, dice=True).to(device)\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "# Create model, optimizer, lr_scheduler, and pass to training function \n",
    "num_classes = len(class_id_name_map.items()) \n",
    "dense_classifier = PointNetSegHead(m = NUM_PARTS, num_points = num_points).to(device) \n",
    "\n",
    "# Optimizer \n",
    "optimizer = optim.SGD(dense_classifier.parameters(), lr = 0.01, momentum = 0.9) \n",
    "\n",
    "train_model(dense_classifier, N_EPOCHS, criterion, optimizer, train_loader, label_str = \n",
    "           'seg_labels', output_name = 'pointnet_seg.pth', Segmentation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:08:31.581395Z",
     "iopub.status.busy": "2025-01-08T11:08:31.580956Z",
     "iopub.status.idle": "2025-01-08T11:08:44.480186Z",
     "shell.execute_reply": "2025-01-08T11:08:44.479101Z",
     "shell.execute_reply.started": "2025-01-08T11:08:31.581350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:12<00:00,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19535148922312126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dense_classifier.load_state_dict(torch.load('pointnet_seg.pth', weights_only = True))\n",
    "dense_classifier.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_dict in tqdm(test_loader, total=len(test_loader)):\n",
    "        x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "        labels = batch_dict['seg_labels'].to(device)\n",
    "        pred, _, _ = dense_classifier(x)\n",
    "        pred_transposed = pred.transpose(2,1)\n",
    "\n",
    "        pred_choice = torch.softmax(pred_transposed, dim=2).argmax(dim=2)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = criterion(pred_transposed, labels, pred_choice)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "evaluation_loss = total_loss / len(test_loader)\n",
    "print(evaluation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on individual items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T11:08:44.482132Z",
     "iopub.status.busy": "2025-01-08T11:08:44.481821Z",
     "iopub.status.idle": "2025-01-08T11:08:44.611713Z",
     "shell.execute_reply": "2025-01-08T11:08:44.610937Z",
     "shell.execute_reply.started": "2025-01-08T11:08:44.482102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"1220px\"\n",
       "    height=\"420\"\n",
       "    src=\"iframe_figures/figure_33.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random test sample\n",
    "test_sample = test_set[np.random.choice(np.arange(len(test_set)))]\n",
    "batch_dict = collate_fn([test_sample])\n",
    "\n",
    "# Get model predictions\n",
    "x = batch_dict['points'].transpose(1, 2).to(device)\n",
    "model_preds, _, _ = dense_classifier(x)\n",
    "pred_part_labels = torch.argmax(model_preds, axis=1).detach().cpu().numpy()[0]\n",
    "\n",
    "points = test_sample['points']\n",
    "part_labels = test_sample['seg_labels']\n",
    "\n",
    "\n",
    "# plot results\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}]], column_widths=[0.5, 0.5],\n",
    "                    subplot_titles=('Part Labels', 'Part Predictions'))\n",
    "\n",
    "# ground truth part labels\n",
    "part_label_plots = plot_pc_data3d(x=points[:,0], y=points[:,1], z=points[:,2], apply_color_gradient=False, \n",
    "                                  color=PART_COLORS[part_labels - 1], marker_size=2)\n",
    "\n",
    "# ground truth part labels\n",
    "pred_part_label_plots = plot_pc_data3d(x=points[:,0], y=points[:,1], z=points[:,2], apply_color_gradient=False, \n",
    "                                  color=PART_COLORS[pred_part_labels - 1], marker_size=2)\n",
    "\n",
    "fig.update_layout(template=\"plotly_dark\", scene=PCD_SCENE, scene2=PCD_SCENE, height = 400, width = 1200,\n",
    "                title='PointNet Segmentation', title_x=0.5, title_y=0.97, margin=dict(r=0, b=0, l=0, t=0))\n",
    "fig.add_trace(part_label_plots, row=1, col=1)\n",
    "fig.add_trace(pred_part_label_plots, row=1, col=2)\n",
    "fig.show(renderer='iframe')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3309985,
     "sourceId": 5757004,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3689970,
     "sourceId": 6399893,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
